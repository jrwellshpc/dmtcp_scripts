#!/bin/bash

#SBATCH -J tensorflow        # Job name
#SBATCH -o tf.%j.o        # Name of stdout output file (%j expands to jobId)
#SBATCH -e tf.%j.e        # Name of standard error file
#SBATCH -N 1              # Total number of nodes requested
#SBATCH -n 1              # Total number of mpi tasks requested
#SBATCH -t 0-00:30:00     # Time (D-HH:MM:SS)
#SBATCH --mem=100000M     # Memory
#SBATCH -p gpu_requeue	  # Partition(s) (separate with commas if using multiple)
#SBATCH --gres=gpu:1      # Request 1 GPU
#SBATCH --gpu-freq=high   # Configure the GPU for high frequency operations

# In dmtcp_restart and dmtcp_launch you will see a -i 360. That means create a checkpoint every 360 seconds. 
# This is a lot of overhead so I'd recommend changing that to 3600 (once an hour) when you are ready.
#
# Lessons Learned
# It can take a long time to create a checkpoint and with a short interval, such as 10, it'll checkpoint itself doing a checkpoint.
# That causes a crash.
#
module load gcc/10.2.0-fasrc01
module load Anaconda3/5.0.1-fasrc01
module load cuda/11.1.0-fasrc01
module load cudnn/8.0.4.30_cuda11.1-fasrc01
source activate tf2_cuda11.1
dmtcp_coordinator -i 600 &
FILE=dmtcp_restart_script.sh
#export DMTCP_DL_PLUGIN=0  # Hack for non-GPU Tensorflow runs
if [ -f "$FILE" ]; then
    export LATEST_CHECKPOINT=$(ls -lart ckpt_*.dmtcp | tail -1 | tr -s ' ' | cut -d ' ' -f9)
    dmtcp_restart -j $LATEST_CHECKPOINT
else
    dmtcp_launch -j python3 -u tf.py
fi
source deactivate
